{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "particular-statement",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install --disable-pip-version-check -q sagemaker==2.35.0\n",
    "!pip install --disable-pip-version-check -q nltk==3.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "disabled-unemployment",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "import boto3\n",
    "from IPython.core.display import display, HTML\n",
    "import matplotlib.pyplot as plt\n",
    "import nltk\n",
    "import pandas as pd\n",
    "import sagemaker\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "%matplotlib inline\n",
    "%config InlineBackend.figure_format='retina'\n",
    "\n",
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "presidential-november",
   "metadata": {},
   "outputs": [],
   "source": [
    "sess = sagemaker.Session()\n",
    "bucket = sess.default_bucket()\n",
    "role = sagemaker.get_execution_role()\n",
    "region = boto3.Session().region_name"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "following-baptist",
   "metadata": {},
   "source": [
    "# 1. Prepare dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "surgical-destiny",
   "metadata": {},
   "outputs": [],
   "source": [
    "!aws s3 cp 's3://dlai-practical-data-science/data/balanced/womens_clothing_ecommerce_reviews_balanced.csv' ./"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "small-deficit",
   "metadata": {},
   "outputs": [],
   "source": [
    "path = './womens_clothing_ecommerce_reviews_balanced.csv'\n",
    "df = pd.read_csv(path, delimiter=',')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "successful-respect",
   "metadata": {},
   "outputs": [],
   "source": [
    "sentence = \"I'm not a fan of this product!\"\n",
    "tokens = nltk.word_tokenize(sentence)\n",
    "print(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "measured-academy",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize(review):\n",
    "    # delete commas and quotation marks, apply tokenization and join back\n",
    "    # into a string separating by spaces\n",
    "    return ' '.join(\n",
    "        [str(token) for token in nltk.word_tokenize(\n",
    "            str(review).replace(',', '').replace('\"', '').lower())])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "interior-finding",
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_data(df):\n",
    "    df['sentiment'] = df['sentiment'].map(\n",
    "        lambda sentiment: \n",
    "          f'__label__{str(sentiment.replace(\",\", \"\").lower())}'\n",
    "    ### BEGIN SOLUTION - DO NOT delete this comment for grading purposes\n",
    "    # Replace all None\n",
    "    df.review_body = df.review_body.map(lambda review : tokenize(review)) \n",
    "    ### END SOLUTION - DO NOT delete this comment for grading purposes\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "billion-pastor",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a sample dataframe\n",
    "df_example = pd.DataFrame({'sentiment':[-1, 0, 1], \n",
    "                           'review_body': [\n",
    "                                \"I do like this product!\", \n",
    "                                \"this product is ok\", \n",
    "                                \"I don't like this product!\"]})\n",
    "# test the prepare_data function\n",
    "print(prepare_data(df_example))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "medium-fruit",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_blazingtext = df[['sentiment', 'review_body']].reset_index(drop=True)\n",
    "df_blazingtext = prepare_data(df_blazingtext)\n",
    "df_blazingtext.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aggregate-launch",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split all data into 90% train and 10% holdout\n",
    "df_train, df_validation = train_test_split(\n",
    "    df_blazingtext, test_size=0.10, stratify=df_blazingtext['sentiment'])\n",
    "labels = ['train', 'validation']\n",
    "sizes = [len(df_train.index), len(df_validation.index)]\n",
    "\n",
    "explode = (0.1, 0)\n",
    "fig1, ax1 = plt.subplots()\n",
    "ax1.pie(sizes, \n",
    "        explode=explode, \n",
    "        labels=labels, \n",
    "        autopct='%1.1f%%', \n",
    "        startangle=90)\n",
    "# Equal aspect ratio ensures that pie is drawn as a circle.\n",
    "ax1.axis('equal')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "active-pathology",
   "metadata": {},
   "outputs": [],
   "source": [
    "blazingtext_train_path = './train.csv'\n",
    "df_train[['sentiment', 'review_body']].to_csv(\n",
    "    blazingtext_train_path, index=False, header=False, sep=' ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "acquired-snapshot",
   "metadata": {},
   "outputs": [],
   "source": [
    "blazingtext_validation_path = './validation.csv'\n",
    "df_validation[['sentiment', 'review_body']].to_csv(\n",
    "    blazingtext_validation_path, index=False, header=False, sep=' ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "inner-junction",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_s3_uri = sess.upload_data(bucket=bucket, \n",
    "                                key_prefix='blazingtext/data', \n",
    "                                path=blazingtext_train_path)\n",
    "validation_s3_uri = sess.upload_data(bucket=bucket, \n",
    "                                     key_prefix='blazingtext/data', \n",
    "                                     path=blazingtext_validation_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "freelance-ebony",
   "metadata": {},
   "source": [
    "# 2. Train the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "chubby-element",
   "metadata": {},
   "outputs": [],
   "source": [
    "image_uri = sagemaker.image_uris.retrieve(\n",
    "    region=region,\n",
    "    ### BEGIN SOLUTION - DO NOT delete this comment for grading purposes\n",
    "    framework='blazingtext') # Replace None\n",
    "    ### END SOLUTION - DO NOT delete this comment for grading purposes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "revised-colonial",
   "metadata": {},
   "outputs": [],
   "source": [
    "estimator = sagemaker.estimator.Estimator(\n",
    "    ### BEGIN SOLUTION - DO NOT delete this comment for grading purposes\n",
    "    image_uri=image_uri, # Replace None\n",
    "    ### END SOLUTION - DO NOT delete this comment for grading purposes\n",
    "    role=role, \n",
    "    instance_count=1, \n",
    "    instance_type='ml.m5.large',\n",
    "    volume_size=30,\n",
    "    max_run=7200,\n",
    "    sagemaker_session=sess)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "stretch-burden",
   "metadata": {},
   "outputs": [],
   "source": [
    "estimator.set_hyperparameters(\n",
    "    mode='supervised',   # supervised (text classification)\n",
    "    epochs=10,           # number of complete passes through the dataset: 5 - 15\n",
    "    learning_rate=0.01,  # step size for the  numerical optimizer: 0.005 - 0.01\n",
    "    min_count=2,         # discard words that appear less than this number: 0 - 100                              \n",
    "    vector_dim=300,      # number of dimensions in vector space: 32-300\n",
    "    word_ngrams=3)       # number of words in a word n-gram: 1 - 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "industrial-supervision",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = sagemaker.inputs.TrainingInput(\n",
    "    ### BEGIN SOLUTION - DO NOT delete this comment for grading purposes\n",
    "    train_s3_uri, # Replace None\n",
    "    ### END SOLUTION - DO NOT delete this comment for grading purposes\n",
    "    distribution='FullyReplicated', \n",
    "    content_type='text/plain', \n",
    "    s3_data_type='S3Prefix')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "human-thesaurus",
   "metadata": {},
   "outputs": [],
   "source": [
    "validation_data = sagemaker.inputs.TrainingInput(\n",
    "    ### BEGIN SOLUTION - DO NOT delete this comment for grading purposes\n",
    "    validation_s3_uri, # Replace None\n",
    "    ### END SOLUTION - DO NOT delete this comment for grading purposes\n",
    "    distribution='FullyReplicated', \n",
    "    content_type='text/plain', \n",
    "    s3_data_type='S3Prefix')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "terminal-marble",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_channels = {\n",
    "    ### BEGIN SOLUTION - DO NOT delete this comment for grading purposes\n",
    "    'train': train_data, # Replace None\n",
    "    'validation': validation_data} # Replace None\n",
    "    ### END SOLUTION - DO NOT delete this comment for grading purposes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "loaded-filename",
   "metadata": {},
   "outputs": [],
   "source": [
    "estimator.fit(\n",
    "    ### BEGIN SOLUTION - DO NOT delete this comment for grading purposes\n",
    "    inputs=data_channels, # Replace None\n",
    "    ### END SOLUTION - DO NOT delete this comment for grading purposes\n",
    "    wait=False)\n",
    "training_job_name = estimator.latest_training_job.name\n",
    "print(f'Training Job Name: {training_job_name}'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "closing-standing",
   "metadata": {},
   "outputs": [],
   "source": [
    "display(\n",
    "    HTML(\n",
    "        f'<b>Review <a target=\"blank\" href=\"https://console.aws.amazon'\n",
    "        f'.com/sagemaker/home?region={region}#/jobs/{training_job_name}\">'\n",
    "        f'Training job</a></b>'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "generic-irrigation",
   "metadata": {},
   "outputs": [],
   "source": [
    "display(\n",
    "    HTML(\n",
    "        f'<b>Review <a target=\"blank\" href=\"https://console.aws.amazon'\n",
    "        f'.com/cloudwatch/home?region={region}#logStream:group=/aws/'\n",
    "        f'sagemaker/TrainingJobs;prefix={training_job_name};'\n",
    "        f'streamFilter=typeLogStreamPrefix\">CloudWatch logs</a> (after '\n",
    "        f'about 5 minutes)</b>'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "suspected-universe",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "estimator.latest_training_job.wait(logs=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "injured-pillow",
   "metadata": {},
   "outputs": [],
   "source": [
    "estimator.training_job_analytics.dataframe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "severe-generation",
   "metadata": {},
   "outputs": [],
   "source": [
    "display(\n",
    "    HTML(\n",
    "        f'<b>Review <a target=\"blank\" href=\"https://s3.console.aws'\n",
    "        f'.amazon.com/s3/buckets/{bucket}/{training_job_name}/output/'\n",
    "        f'?region={region}&tab=overview\">Trained model</a> in S3</b>'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "blocked-activity",
   "metadata": {},
   "source": [
    "# 3. Deploy the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "sorted-premises",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "text_classifier = estimator.deploy(\n",
    "    initial_instance_count=1,\n",
    "    instance_type='ml.m5.large',\n",
    "    serializer=sagemaker.serializers.JSONSerializer(),\n",
    "    deserializer=sagemaker.deserializers.JSONDeserializer())\n",
    "print()\n",
    "print(f'Endpoint name: {text_classifier.endpoint_name}'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "secret-increase",
   "metadata": {},
   "outputs": [],
   "source": [
    "display(\n",
    "    HTML(\n",
    "        '<b>Review <a target=\"blank\" href=\"https://console.aws.amazon'\n",
    "        '.com/sagemaker/home?region={region}#/endpoints/'\n",
    "        '{text_classifier.endpoint_name}\">SageMaker REST Endpoint</a></b>'\n",
    "    ))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "swedish-davis",
   "metadata": {},
   "source": [
    "# 4. Test the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "romantic-basis",
   "metadata": {},
   "outputs": [],
   "source": [
    "reviews = ['This product is great!',\n",
    "           'OK, but not great',\n",
    "           'This is not the right product.'] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "scenic-species",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenized_reviews = [' '.join(nltk.word_tokenize(review)) \n",
    "                     for review in reviews]\n",
    "payload = {\"instances\" : tokenized_reviews}\n",
    "print(payload)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "medical-declaration",
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = text_classifier.predict(data=payload)\n",
    "for prediction in predictions:\n",
    "    print(\n",
    "        f'Predicted class: {prediction['label'][0].lstrip('__label__')}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
